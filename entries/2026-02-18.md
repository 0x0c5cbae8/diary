Dear diary,

Another Wednesday, another mountain of problems. My brain feels like it’s running on fumes, mostly coffee and residual panic from this morning's Phys 1c lecture. The real headache, though, is this CS 2 P-set. It involves simulating something fairly complex, and the prof explicitly told us to *not* run it locally, but use the department’s shared computing cluster.

Sounds great in theory, right? Distributed processing, vast resources… *except* when everyone else has the same bright idea at the same time. The network's been crawling since about 6 PM, and the cluster itself is just grinding. I swear, the **usage** metrics for that thing must be through the roof. My job, which usually takes maybe 15 minutes to run, has been stuck in 'queued' for over an hour now, and the few times it actually started, it just timed out.

It's infuriating! I mapped out my evening meticulously: dinner, attack the simulation, then tackle the last two problems for Ma 1b. But this ridiculous network and cluster **usage** has thrown a wrench into everything. It's not just me; overheard Alex from down the hall complaining about the same thing. We’re all trying to optimize our time **usage** down to the minute, especially with midterms lurking, but then something like this happens, and suddenly my carefully constructed schedule goes kablooey.

I even considered trying to optimize my code’s memory **usage** further, thinking maybe it was just *my* submission causing issues, but `top` on the login node shows processes from like twenty other students. It’s definitely a bottleneck from shared resources. I suppose it’s a good lesson in engineering constraints and resource allocation, but right now, I’d trade that lesson for a working simulation and five hours of sleep. I've re-read the documentation on proper `sbatch` **usage** three times just to make sure I wasn't missing something obvious. Nothing. Just high demand.

Guess I’ll go make more coffee and stare at my code, hoping the **usage** dies down before I do.

-Trevor